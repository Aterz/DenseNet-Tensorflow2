{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190604\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, ZeroPadding2D, Dense, Dropout, Activation, Convolution2D, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "DATASET_NAME = \"lsa16\"\n",
    "\n",
    "data = hd.load(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data[0]\n",
    "labels = data[1]['y']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.10,\n",
    "    height_shift_range=0.10,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from densenet import densenet_model\n",
    "growth_rate=128\n",
    "nb_layers=[6,12]\n",
    "\n",
    "model = densenet_model(growth_rate=growth_rate, nb_layers=nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "models_directory = 'trained-models'\n",
    "results_directory = 'results'\n",
    "results = 'epoch,loss,accuracy,test_loss,test_accuracy\\n'\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "log_freq = 10\n",
    "save_freq = 40\n",
    "identifier = \"tiny-{}-growth-{}-densenet-vanilla-data-\".format('-'.join([str(i) for i in nb_layers] ), growth_rate)\n",
    "             + now.strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "train_summary_writer = tf.summary.create_file_writer('summaries/train/'+ identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('summaries/test/'+ identifier)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    batches = 0\n",
    "    for images, labels in datagen.flow(x_train, y_train, batch_size=32):\n",
    "        train_step(images, labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_datagen.flow(x_test, y_test, batch_size=32, shuffle=False):\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_test) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        results += '{},{},{},{},{}\\n'.format(epoch,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100)\n",
    "        \n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}'.format(epoch,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()           \n",
    "        \n",
    "        \n",
    "    \n",
    "    if (epoch % save_freq == 0):\n",
    "        if not os.path.exists(models_directory):\n",
    "            os.makedirs(models_directory)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(models_directory+\"/{}_epoch{}.h5\".format(identifier,epoch))\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "file = open(results_directory+'/results-'+ identifier + '.csv','w') \n",
    "file.write(results) \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
